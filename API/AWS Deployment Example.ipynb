{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "startTimeObj = datetime.now()\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "ec2_resource = boto3.resource(\"ec2\")\n",
    "ec2_client = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load AWS_functions.py\n",
    "import uuid\n",
    "import subprocess    \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import paramiko\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "def aws_create_bucket_name(bucket_name):\n",
    "    \"\"\"Creta a unique bucket name\n",
    "    Keyword arguments:\n",
    "    bucket_name: string\n",
    "    return name + uuid string\n",
    "    \"\"\"\n",
    "    return '-'.join([bucket_name, str(uuid.uuid4())])\n",
    "\n",
    "def aws_create_bucket(bucket_name, s3_connection, current_region):\n",
    "    \"\"\"Create an AWS Bucket\n",
    "    Keyword arguments:\n",
    "    bucket name: string\n",
    "    s3_connection: boto3 S3 Resource\n",
    "    return bucket_name, bucket_response\n",
    "    \"\"\"\n",
    "    # Create Configurations\n",
    "    configs = {}\n",
    "    if current_region != \"us-east-1\":\n",
    "        config[\"LocationConstraint\"] = current_region\n",
    "\n",
    "    bucket_name = aws_create_bucket_name(bucket_name)\n",
    "    \n",
    "    if current_region == \"us-east-1\":\n",
    "        bucket_response = s3_connection.create_bucket(\n",
    "            Bucket=bucket_name)\n",
    "    else:\n",
    "        bucket_response = s3_connection.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={\"LocationConstraint\":current_region})\n",
    "        \n",
    "    print(bucket_name, current_region)\n",
    "    return bucket_name, bucket_response\n",
    "\n",
    "def aws_upload_files_to_bucket(path_to_src, bucket_name):\n",
    "    \"\"\"Use AWS CLI to sync a local folder to AWS Bucket\"\"\"\n",
    "    path_to_s3 = \"s3://\" + bucket_name\n",
    "\n",
    "    try:\n",
    "        result = os.popen('aws s3 sync '+path_to_src+' '+path_to_s3+' --exclude \"model/*\" --acl private').read()\n",
    "        print(result)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"upload failed\")\n",
    "        return False\n",
    "\n",
    "def aws_list_objects(bucket_name, s3_resource):\n",
    "    bucket=s3_resource.Bucket(bucket_name)\n",
    "    res = []\n",
    "    for obj_version in bucket.object_versions.all():\n",
    "        res.append({\n",
    "            'file_name': obj_version.object_key,\n",
    "            'VersionId': obj_version.id\n",
    "        })\n",
    "    return res, bucket\n",
    "\n",
    "def aws_delete_all_objects(bucket_name, s3_resource):\n",
    "    \"\"\"Delete all objects inside a bucket\"\"\"\n",
    "    resources, bucket = aws_list_objects(bucket_name, s3_resource)\n",
    "    bucket.delete_objects(Delete={'Objects': resources})\n",
    "    return bucket\n",
    "\n",
    "def aws_delete_bucket(bucket_name, s3_resource):\n",
    "    \"\"\"delete bucket and all its content\"\"\"\n",
    "    bucket = aws_delete_all_objects(bucket_name, s3_resource)\n",
    "    bucket.delete()\n",
    "\n",
    "def aws_list_buckets(bucket_name, s3_resource):\n",
    "    return s3_resource.buckets.all()\n",
    "\n",
    "def aws_download_objects(path_to_src, bucket_name):\n",
    "    \"\"\"Use AWS CLI to sync a local folder to AWS Bucket\"\"\"\n",
    "    path_to_s3 = \"s3://\" + bucket_name\n",
    "\n",
    "    try:\n",
    "        result = os.popen('aws s3 sync '+path_to_s3+' '+path_to_src).read()\n",
    "        print(result)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"upload failed\")\n",
    "        return False  \n",
    "\n",
    "def aws_create_ec2_name(bucket_prefix):\n",
    "    \"\"\"Create a unique ec2 name\"\"\"\n",
    "    return '-'.join([bucket_prefix, str(uuid.uuid4())])\n",
    "\n",
    "def aws_user_data_script(bucket_name):\n",
    "    user_data = [\n",
    "        \"#cloud-boothook\",\n",
    "        \"#!/bin/bash\",\n",
    "        \"yum update -q -y\",\n",
    "        \"yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel -q -y\",\n",
    "        \"git clone https://github.com/s3fs-fuse/s3fs-fuse.git /tmp/s3fs-fuse\",\n",
    "        \"cd /tmp/s3fs-fuse\",\n",
    "        \"./autogen.sh\",\n",
    "        \"./configure --prefix=/usr --with-openssl\",\n",
    "        \"make\",\n",
    "        \"make install\",\n",
    "        'echo \"' + os.getenv(\"AWS_ACCESS_ID\") + ':' + os.getenv(\"AWS_SECRET_ACCESS_KEY\") + '\" > /tmp/passwd-s3fs',\n",
    "        \"mv -f /tmp/passwd-s3fs /etc\",\n",
    "        \"chmod 640 /etc/passwd-s3fs\",\n",
    "        \"mkdir -p /mys3bucket\",\n",
    "        \"chown ec2-user:ec2-user /mys3bucket\",\n",
    "        \"s3fs \" + bucket_name + \" -o use_cache=/tmp -o uid=500 -o mp_umask=002 -o multireq_max=5 -o allow_other /mys3bucket\",\n",
    "        # \"while read requirement; do conda install --yes $requirement; done < /mys3bucket/requirements.txt\"\n",
    "    ]\n",
    "    return \"\\n\".join(user_data)\n",
    "\n",
    "def aws_create_ec2_instance(ec2_name, security_group, user_data, ec2_resource, image_id = \"ami-062c42cbecc1d5ec0\", instance_type=\"t2.medium\"):\n",
    "    pem_key =  os.getenv(\"AWS_PEM_KEY\")\n",
    "    ec2_name = aws_create_ec2_name(ec2_name)\n",
    "\n",
    "    instances = ec2_resource.create_instances(\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'instance',\n",
    "                'Tags': [\n",
    "                    {\n",
    "                        'Key': 'Name',\n",
    "                        'Value': ec2_name\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        ImageId=image_id,\n",
    "        MinCount=1,\n",
    "        MaxCount=1,\n",
    "        InstanceType=instance_type,\n",
    "    #     InstanceType='t2.medium',\n",
    "        KeyName=pem_key,\n",
    "        SecurityGroupIds=security_group, #Bring your own!\n",
    "        UserData=user_data\n",
    "    )\n",
    "\n",
    "    return instances[0]\n",
    "\n",
    "def aws_stop_ec2(instance_id, ec2_client):\n",
    "    return ec2_client.stop_instances(\n",
    "        InstanceIds=[\n",
    "            instance_id,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def exec_ssh_cmd(public_dns_name = \"\", commands = \"\"):\n",
    "    pem_key = os.getenv(\"AWS_PEM_KEY\")\n",
    "    pem_location = os.getenv(\"AWS_PEM_LOCATION\")\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    privkey = paramiko.RSAKey.from_private_key_file(pem_location+pem_key+'.pem')\n",
    "    ssh.connect(public_dns_name,username=\"ec2-user\",pkey=privkey)\n",
    "    \n",
    "    result = []\n",
    "    for cmd in commands:\n",
    "        stdin, stdout, stderr = ssh.exec_command(cmd)\n",
    "        stdin.flush()\n",
    "        data = stdout.read().splitlines()\n",
    "        data = [l.decode(\"utf-8\") for l in data]\n",
    "\n",
    "        result.append({\"cmd\":cmd,\"response\":data})\n",
    "\n",
    "    ssh.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Following Workflow\n",
    "\n",
    "### [File Storage](#File_Storage)\n",
    "* [Create](#File_Storage_Create) an **S3 Bucket**\n",
    "* [Load](#File_Storage_Load) csv file and script to execute\n",
    "* [Delete](#File_Storage_Delete) object and content\n",
    "* [List](#File_Storage_List) objects\n",
    "\n",
    "### [Execute Code](#Execute_Code)\n",
    "#### [EC2](#Execute_Code_EC2) Miniconda Image\n",
    "* [Create](#Execute_Code_EC2_Create) the EC2 Instance\n",
    "* [Mount](#Execute_Code_EC2_Mount) S3 Storage\n",
    "* [Execute](#Execute_Code_EC2_Script) Script\n",
    "* [Stop](#Execute_Code_EC2_Stop) \\ Terminate server\n",
    "\n",
    "### [Deploy our Model](#Deploy-Model)\n",
    "\n",
    "### Requirements\n",
    "pycloud environment\n",
    "```\n",
    "conda env create -f pycloud.env.ymp --force\n",
    "```\n",
    "\n",
    "From Command line execute `aws configure` in order to setup your AWS Access Key and AWS Secret Key\n",
    "\n",
    "Local Test Code is located under `example_src`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top](#top)\n",
    "## File Storage<a id=\"File_Storage\"></a>\n",
    "Code to create an S3 Storage\n",
    "https://realpython.com/python-boto3-aws-s3/#creating-a-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Object<a id=\"File_Storage_Create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run AWS_functions.py\n",
    "\n",
    "# bucket_name, first_response = aws_create_bucket(\n",
    "#     bucket_name='iris-train', \n",
    "#     s3_connection=s3_resource.meta.client,\n",
    "#     current_region=session.region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files into S3 Object<a id=\"File_Storage_Load\"></a>\n",
    "There is no native folder sync within Python SDK, so using aws command to solve for this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run AWS_functions.py\n",
    "\n",
    "# success = aws_upload_files_to_bucket(\n",
    "#     path_to_src=\"../example_src/iris\", \n",
    "#     bucket_name=bucket_name)\n",
    "\n",
    "# print(\"Uploaded was a\", success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to EC2](#Execute_Code_EC2)\n",
    "[Go to SageMaker](#Execute_Code_SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete S3 Object and Content<a id=\"File_Storage_Delete\"></a>\n",
    "Remove all resources and delete bucket!<br>\n",
    "**This does not back-up anything!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %run AWS_functions.py\n",
    "\n",
    "# aws_delete_bucket(bucket_name, s3_resource)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List S3 Buckets<a id=File_Storage_List></a>\n",
    "Quickly list and clean up all buckets created by with iris-trian in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122\n",
      "iris-train-e41a03ef-6946-44a0-b326-b6784f36a9f3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3_resource = boto3.resource('s3')\n",
    "for bucket in s3_resource.buckets.all():\n",
    "  if \"iris-train\" in bucket.name:\n",
    "    print(bucket.name)\n",
    "#     aws_delete_bucket(bucket.name, s3_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top](#top)\n",
    "## Execute Code<a id=Execute_Code></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Instance<a id=Execute_Code_EC2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create EC2 Instance<a id=Execute_Code_EC2_Create></a>\n",
    "https://blog.ipswitch.com/how-to-create-an-ec2-instance-with-python\n",
    "\n",
    "You need to bring your own:\n",
    "* Security Group\n",
    "* pem key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building the following EC2\n",
    "* MiniConda - ami-062c42cbecc1d5ec0\n",
    "* t2.medium\n",
    "\n",
    "I built my own security group and granted ssh access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a bash script to create the S3 Mount, go [here](#Execute_Code_EC2_Mount) to see details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount S3 onto EC2 Instance<a id=Execute_Code_EC2_Mount></a>\n",
    "https://cloudkul.com/blog/mounting-s3-bucket-linux-ec2-instance/\n",
    "\n",
    "Using existing EC2 in AWS\n",
    "Need to leverage API to create EC2 and mount determining setup\n",
    "\n",
    "**Required setup on EC2**\n",
    "```\n",
    "sudo yum update\n",
    "sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel\n",
    "git clone https://github.com/s3fs-fuse/s3fs-fuse.git\n",
    "cd s3fs-fuse\n",
    "./autogen.sh\n",
    "./configure --prefix=/usr --with-openssl\n",
    "make\n",
    "sudo make install\n",
    "```\n",
    "\n",
    "You must create an IAM role for S3 Mounting, for sake of simplicity, i'm using my Admin IAM Access\n",
    "```\n",
    "sudo touch /etc/passwd-s3fs\n",
    "sudo vim /etc/passwd-s3fs\n",
    "```\n",
    "Provide `Your_accesskey:Your_secretkey` inside the file\n",
    "```\n",
    "sudo chmod 640 /etc/passwd-s3fs\n",
    "```\n",
    "\n",
    "Let's mount it!, replace iris-trainc0e3588c-d9bb-4699-821c-1883670ace42 with your bucket name\n",
    "uid=500 is ec2-user account\n",
    "```\n",
    "sudo mkdir /mys3bucket\n",
    "sudo chown ec2-user:ec2-user /mys3bucket\n",
    "s3fs iris-trainc0e3588c-d9bb-4699-821c-1883670ace42 -o use_cache=/tmp -o allow_other -o uid=500 -o mp_umask=002 -o multireq_max=5 /mys3bucket\n",
    "```\n",
    "\n",
    "Validate\n",
    "```\n",
    "df -Th\n",
    "```\n",
    "\n",
    "Mount at reboot\n",
    "```\n",
    "vi /etc/rc.local\n",
    "/usr/bin/s3fs iris-trainc0e3588c-d9bb-4699-821c-1883670ace42 -o use_cache=/tmp -o allow_other -o uid=500 -o mp_umask=002 -o multireq_max=5 /mys3bucket\n",
    "```\n",
    "**or** add it to the User Data at execution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance id:  i-0a24097ed20c3ee63\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "from IPython.display import display\n",
    "import socket\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# create a new EC2 instance\n",
    "user_data = aws_user_data_script(bucket_name)\n",
    "\n",
    "instance = aws_create_ec2_instance(\n",
    "    ec2_name=\"iris-train\", \n",
    "    security_group=['sg-0d24aec64507df8b5'], \n",
    "    user_data=user_data, \n",
    "    ec2_resource=ec2_resource,\n",
    "    image_id = \"ami-062c42cbecc1d5ec0\", \n",
    "    instance_type=\"t2.medium\")\n",
    "\n",
    "print(\"instance id: \",instance.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait till instance state changes to running\n",
      "Instance State Up, waiting for boot-up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'instance is still loading retrying . . . . '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance is UP & accessible on port 22, the IP address is:   54.159.6.87\n"
     ]
    }
   ],
   "source": [
    "#Provide status when instance is finally up!\n",
    "retries = 10\n",
    "retry_delay = 10\n",
    "retry_count = 0\n",
    "\n",
    "print(\"Wait till instance state changes to running\")\n",
    "instance.wait_until_running()\n",
    "instance = ec2_resource.Instance(id=instance.id)\n",
    "print(\"Instance State Up, waiting for boot-up\")\n",
    "\n",
    "waiting_status = \"instance is still loading retrying . . . \"\n",
    "dh = display(waiting_status,display_id=True)\n",
    "\n",
    "while retry_count <= retries:\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    result = sock.connect_ex((instance.public_ip_address,22))\n",
    "    if result == 0:\n",
    "        print(\"Instance is UP & accessible on port 22, the IP address is:  \",instance.public_ip_address)\n",
    "        break\n",
    "    else:\n",
    "        if len(waiting_status) < 50:\n",
    "            waiting_status += \". \"\n",
    "        else:\n",
    "            waiting_status = waiting_status[0:41]\n",
    "\n",
    "        dh.update(waiting_status)\n",
    "        time.sleep(retry_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands via SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I can tell miniconda app happens after UserData is complete, thus no installing conda environment\n",
    "\n",
    "We will execute the following command through ssh client via root access\n",
    "```\n",
    "sudo su\n",
    "while read requirement; do conda install --yes $requirement; done < /mys3bucket/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /opt/conda:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "asn1crypto                0.24.0                   py37_0  \n",
      "blas                      1.0                         mkl  \n",
      "ca-certificates           2019.10.16                    0  \n",
      "certifi                   2019.9.11                py37_0  \n",
      "cffi                      1.11.5           py37he75722e_1  \n",
      "chardet                   3.0.4                    py37_1  \n",
      "conda                     4.5.12                   py37_0  \n",
      "conda-env                 2.6.0                         1  \n",
      "cryptography              2.4.2            py37h1ba5d50_0  \n",
      "idna                      2.8                      py37_0  \n",
      "intel-openmp              2019.4                      243  \n",
      "joblib                    0.14.0                     py_0  \n",
      "libedit                   3.1.20181209         hc058e9b_0  \n",
      "libffi                    3.2.1                hd88cf55_4  \n",
      "libgcc-ng                 9.1.0                hdf63c60_0  \n",
      "libgfortran-ng            7.3.0                hdf63c60_0  \n",
      "libstdcxx-ng              8.2.0                hdf63c60_1  \n",
      "mkl                       2019.4                      243  \n",
      "mkl-service               2.3.0            py37he904b0f_0  \n",
      "mkl_fft                   1.0.15           py37ha843d7b_0  \n",
      "mkl_random                1.1.0            py37hd6b4f25_0  \n",
      "ncurses                   6.1                  he6710b0_1  \n",
      "numpy                     1.17.3           py37hd14ec0e_0  \n",
      "numpy-base                1.17.3           py37hde5b4d6_0  \n",
      "openssl                   1.0.2t               h7b6447c_1  \n",
      "pandas                    0.25.3           py37he6710b0_0  \n",
      "pip                       19.3.1                   py37_0  \n",
      "pycosat                   0.6.3            py37h14c3975_0  \n",
      "pycparser                 2.19                     py37_0  \n",
      "pyopenssl                 18.0.0                   py37_0  \n",
      "pysocks                   1.6.8                    py37_0  \n",
      "python                    3.7.0                h6e4f718_3  \n",
      "python-dateutil           2.8.1                      py_0  \n",
      "pytz                      2019.3                     py_0  \n",
      "readline                  7.0                  h7b6447c_5  \n",
      "requests                  2.21.0                   py37_0  \n",
      "ruamel_yaml               0.15.46          py37h14c3975_0  \n",
      "scikit-learn              0.21.3           py37hd81dba3_0  \n",
      "scipy                     1.3.1            py37h7c811a0_0  \n",
      "setuptools                42.0.1                   py37_0  \n",
      "six                       1.12.0                   py37_0  \n",
      "sqlite                    3.30.1               h7b6447c_0  \n",
      "tk                        8.6.8                hbc83047_0  \n",
      "urllib3                   1.24.1                   py37_0  \n",
      "wheel                     0.33.6                   py37_0  \n",
      "xz                        5.2.4                h14c3975_4  \n",
      "yaml                      0.1.7                had09818_2  \n",
      "zlib                      1.2.11               h7b6447c_3  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "commands = [\n",
    "    \"mkdir /mys3bucket/output\",\n",
    "    \"mkdir /mys3bucket/model\",\n",
    "    \"echo 'for requirement in `cat /mys3bucket/requirements.txt` ; do  conda install --yes  ${requirement}  ;  done' > /tmp/setup.sh\",\n",
    "    \"chmod +x /tmp/setup.sh\",\n",
    "    \"sudo  -i /tmp/setup.sh\",\n",
    "    \"conda list\"\n",
    "]\n",
    "\n",
    "ssh_result = exec_ssh_cmd(instance.public_ip_address,commands)\n",
    "\n",
    "#Print the last line conda list\n",
    "[print(l) for l in ssh_result[5][\"response\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Python Script<a id=Execute_Code_EC2_Script></a>\n",
    "\n",
    "Execute the train.py file!\n",
    "```\n",
    "cd /mys3bucket\n",
    "python train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "trian model\n",
      "save model\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "ssh_result = exec_ssh_cmd(\n",
    "    public_dns_name = instance.public_ip_address,\n",
    "    commands = [\"cd /mys3bucket/; python iris_train.py\"])\n",
    "\n",
    "for val in ssh_result[0][\"response\"]:\n",
    "    print(val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for S3 to refresh then download and kill instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2.2 KiB/~2.2 KiB (6.1 KiB/s) with ~1 file(s) remaining (calculating...)\n",
      "download: s3://iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122/iris_train.py to ../example_src/iris/iris_train.py\n",
      "Completed 2.2 KiB/~2.2 KiB (6.1 KiB/s) with ~0 file(s) remaining (calculating...)\n",
      "Completed 10.8 KiB/10.8 KiB (7.9 KiB/s) with 1 file(s) remaining                 \n",
      "download: s3://iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122/model/iris-rf.pkl to ../example_src/iris/model/iris-rf.pkl\n",
      "\n",
      "{'StoppingInstances': [{'CurrentState': {'Code': 64, 'Name': 'stopping'}, 'InstanceId': 'i-0a24097ed20c3ee63', 'PreviousState': {'Code': 16, 'Name': 'running'}}], 'ResponseMetadata': {'RequestId': '9da6c09b-5f3b-4d8d-88ad-2bece8bf9624', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8', 'content-length': '579', 'date': 'Tue, 26 Nov 2019 02:08:12 GMT', 'server': 'AmazonEC2'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "while True:\n",
    "    resources, bucket = aws_list_objects(\n",
    "        bucket_name=bucket_name,\n",
    "        s3_resource=s3_resource)\n",
    "    models = [ res[\"file_name\"] for res in resources if \"model/iris\" in res[\"file_name\"]]\n",
    "    if len(models) > 0:\n",
    "        aws_download_objects(\n",
    "            path_to_src=\"../example_src/iris\",\n",
    "            bucket_name=bucket_name\n",
    "        )\n",
    "        \n",
    "        response = aws_stop_ec2(\n",
    "        instance_id = instance.id, \n",
    "        ec2_client = ec2_client)\n",
    "\n",
    "        print( response )\n",
    "        \n",
    "        break\n",
    "    time.sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Elapse Time 0:06:05.705061\n"
     ]
    }
   ],
   "source": [
    "endTimeObj = datetime.now()\n",
    "\n",
    "print(\"Total Elapse Time\", endTimeObj - startTimeObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model\n",
    "\n",
    "https://medium.com/@patrickmichelberger/how-to-deploy-a-serverless-machine-learning-microservice-with-aws-lambda-aws-api-gateway-and-d5b8cbead846\n",
    "\n",
    "We used a mircoserver flask and zappa to automate lambda, s3, and api gateway\n",
    "\n",
    "*Assumptions*\n",
    "* Flask app created in the project folder under api folder and works locally\n",
    "* Pickle \\ joblib file uploaded to s3\n",
    "* virtualenv installed `pip install virtualenv`\n",
    "\n",
    "1. Setup a lambda virtualenv from your project folder\n",
    "```\n",
    "cd example_src\\iris\n",
    "virtualenv lambda\n",
    "source lambda/bin/activate\n",
    "pip install flask zappa sklearn numpy scipy\n",
    "```\n",
    "2. Test your flask locally by running `python api/app.py`\n",
    "3. Initialize zappa by `zappa init`\n",
    "    * Environment: dev\n",
    "    * S3 Bucket: use default\n",
    "    * App Function: use default `api.app.app`\n",
    "    * Globally: n\n",
    "*Should look like this*\n",
    "```\n",
    "{\n",
    "    \"dev\": {\n",
    "        \"app_function\": \"api.app.app\",\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"profile_name\": \"default\",\n",
    "        \"project_name\": \"iris\",\n",
    "        \"runtime\": \"python3.7\",\n",
    "        \"s3_bucket\": \"zappa-telpd5in0\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "4) Deploy zappa \n",
    "```zappa deploy dev```<br>\n",
    "5) Test Model<br>\n",
    "```\n",
    "curl -d '{\"data\":[[4, 2.1,1,0.4], [6.2, 1,3,2]]}' -X POST https://sgj5uofirg.execute-api.us-east-1.amazonaws.com/dev\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
