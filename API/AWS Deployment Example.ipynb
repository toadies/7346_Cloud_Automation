{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paramiko'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Data Science/7346_Cloud_Automation/API/AWS_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mparamiko\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../.env\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paramiko'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "%run AWS_functions.py\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "ec2_resource = boto3.resource(\"ec2\")\n",
    "ec2_client = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Following Workflow\n",
    "\n",
    "### [File Storage](#File_Storage)\n",
    "* [Create](#File_Storage_Create) an **S3 Bucket**\n",
    "* [Load](#File_Storage_Load) csv file and script to execute\n",
    "* [Delete](#File_Storage_Delete) object and content\n",
    "* [List](#File_Storage_List) objects\n",
    "\n",
    "### [Execute Code](#Execute_Code)\n",
    "#### [EC2](#Execute_Code_EC2) Miniconda Image\n",
    "* [Create](#Execute_Code_EC2_Create) the EC2 Instance\n",
    "* [Mount](#Execute_Code_EC2_Mount) S3 Storage\n",
    "* [Execute](#Execute_Code_EC2_Script) Script\n",
    "* [Stop](#Execute_Code_EC2_Stop) \\ Terminate server\n",
    "\n",
    "#### [SageMaker](#Execute_Code_SM) Instance\n",
    "* [Create](#Execute_Code_SM_Create) SageView\n",
    "* Mount S3 Storage\n",
    "* Execute Script\n",
    "\n",
    "### Requirements\n",
    "pycloud environment\n",
    "```\n",
    "conda env create -f pycloud.env.ymp --force\n",
    "```\n",
    "\n",
    "From Command line execute `aws configure` in order to setup your AWS Access Key and AWS Secret Key\n",
    "\n",
    "Local Test Code is located under `example_src`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top](#top)\n",
    "## File Storage<a id=\"File_Storage\"></a>\n",
    "Code to create an S3 Storage\n",
    "https://realpython.com/python-boto3-aws-s3/#creating-a-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Object<a id=\"File_Storage_Create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paramiko'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Data Science/7346_Cloud_Automation/API/AWS_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mparamiko\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../.env\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paramiko'"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'aws_create_bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bcb2f83d9094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AWS_functions.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m bucket_name, first_response = aws_create_bucket(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iris-train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ms3_connection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms3_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aws_create_bucket' is not defined"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "bucket_name, first_response = aws_create_bucket(\n",
    "    bucket_name='iris-train', \n",
    "    s3_connection=s3_resource.meta.client,\n",
    "    current_region=session.region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files into S3 Object<a id=\"File_Storage_Load\"></a>\n",
    "There is no native folder sync within Python SDK, so using aws command to solve for this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 3.6 KiB/5.7 KiB (23.0 KiB/s) with 3 file(s) remaining\n",
      "upload: ../example_src/iris/data/iris.csv to s3://iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122/data/iris.csv\n",
      "Completed 3.6 KiB/5.7 KiB (23.0 KiB/s) with 2 file(s) remaining\n",
      "Completed 3.6 KiB/5.7 KiB (13.7 KiB/s) with 2 file(s) remaining\n",
      "upload: ../example_src/iris/requirements.txt to s3://iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122/requirements.txt\n",
      "Completed 3.6 KiB/5.7 KiB (13.7 KiB/s) with 1 file(s) remaining\n",
      "Completed 5.7 KiB/5.7 KiB (19.6 KiB/s) with 1 file(s) remaining\n",
      "upload: ../example_src/iris/iris_train.py to s3://iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122/iris_train.py\n",
      "\n",
      "Uploaded was a True\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "success = aws_upload_files_to_bucket(\n",
    "    path_to_src=\"../example_src/iris\", \n",
    "    bucket_name=bucket_name)\n",
    "\n",
    "print(\"Uploaded was a\", success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to EC2](#Execute_Code_EC2)\n",
    "[Go to SageMaker](#Execute_Code_SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete S3 Object and Content<a id=\"File_Storage_Delete\"></a>\n",
    "Remove all resources and delete bucket!<br>\n",
    "**This does not back-up anything!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Key': '.DS_Store', 'VersionId': 'null'}, {'Key': '.ipynb_checkpoints/Untitled-checkpoint.ipynb', 'VersionId': 'null'}, {'Key': 'Untitled.ipynb', 'VersionId': 'null'}, {'Key': 'data/.DS_Store', 'VersionId': 'null'}, {'Key': 'data/chicagoCrimes10k.csv', 'VersionId': 'null'}, {'Key': 'data/training/iris.csv', 'VersionId': 'null'}, {'Key': 'model/HasDetections_GridSearch_RF_final.pkl', 'VersionId': 'null'}, {'Key': 'model/iris-randomforest.pkl', 'VersionId': 'null'}, {'Key': 'notebooks/.ipynb_checkpoints/ChicagoCrime-RF-checkpoint.ipynb', 'VersionId': 'null'}, {'Key': 'notebooks/ChicagoCrime-RF.ipynb', 'VersionId': 'null'}, {'Key': 'output/failure', 'VersionId': 'null'}, {'Key': 'requirements.txt', 'VersionId': 'null'}, {'Key': 'requirements.yml', 'VersionId': 'null'}, {'Key': 'sm_train.py', 'VersionId': 'null'}, {'Key': 'train.py', 'VersionId': 'null'}]\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "aws_delete_bucket(bucket_name, s3_resource)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List S3 Buckets<a id=File_Storage_List></a>\n",
    "Quickly list and clean up all buckets created by with iris-trian in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3_resource = boto3.resource('s3')\n",
    "for bucket in s3_resource.buckets.all():\n",
    "  if \"iris-train\" in bucket.name:\n",
    "    print(bucket.name)\n",
    "#     aws_delete_bucket(bucket.name, s3_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top](#top)\n",
    "## Execute Code<a id=Execute_Code></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Instance<a id=Execute_Code_EC2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create EC2 Instance<a id=Execute_Code_EC2_Create></a>\n",
    "https://blog.ipswitch.com/how-to-create-an-ec2-instance-with-python\n",
    "\n",
    "You need to bring your own:\n",
    "* Security Group\n",
    "* pem key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building the following EC2\n",
    "* MiniConda - ami-062c42cbecc1d5ec0\n",
    "* t2.medium\n",
    "\n",
    "I built my own security group and granted ssh access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a bash script to create the S3 Mount, go [here](#Execute_Code_EC2_Mount) to see details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount S3 onto EC2 Instance<a id=Execute_Code_EC2_Mount></a>\n",
    "https://cloudkul.com/blog/mounting-s3-bucket-linux-ec2-instance/\n",
    "\n",
    "Using existing EC2 in AWS\n",
    "Need to leverage API to create EC2 and mount determining setup\n",
    "\n",
    "**Required setup on EC2**\n",
    "```\n",
    "sudo yum update\n",
    "sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel\n",
    "git clone https://github.com/s3fs-fuse/s3fs-fuse.git\n",
    "cd s3fs-fuse\n",
    "./autogen.sh\n",
    "./configure --prefix=/usr --with-openssl\n",
    "make\n",
    "sudo make install\n",
    "```\n",
    "\n",
    "You must create an IAM role for S3 Mounting, for sake of simplicity, i'm using my Admin IAM Access\n",
    "```\n",
    "sudo touch /etc/passwd-s3fs\n",
    "sudo vim /etc/passwd-s3fs\n",
    "```\n",
    "Provide `Your_accesskey:Your_secretkey` inside the file\n",
    "```\n",
    "sudo chmod 640 /etc/passwd-s3fs\n",
    "```\n",
    "\n",
    "Let's mount it!, replace iris-trainc0e3588c-d9bb-4699-821c-1883670ace42 with your bucket name\n",
    "uid=500 is ec2-user account\n",
    "```\n",
    "sudo mkdir /mys3bucket\n",
    "sudo chown ec2-user:ec2-user /mys3bucket\n",
    "s3fs iris-trainc0e3588c-d9bb-4699-821c-1883670ace42 -o use_cache=/tmp -o allow_other -o uid=500 -o mp_umask=002 -o multireq_max=5 /mys3bucket\n",
    "```\n",
    "\n",
    "Validate\n",
    "```\n",
    "df -Th\n",
    "```\n",
    "\n",
    "Mount at reboot\n",
    "```\n",
    "vi /etc/rc.local\n",
    "/usr/bin/s3fs iris-trainc0e3588c-d9bb-4699-821c-1883670ace42 -o use_cache=/tmp -o allow_other -o uid=500 -o mp_umask=002 -o multireq_max=5 /mys3bucket\n",
    "```\n",
    "**or** add it to the User Data at execution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance id:  i-0c4e0a21c3f6164a9\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "from IPython.display import display\n",
    "import socket\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# create a new EC2 instance\n",
    "user_data = aws_user_data_script(bucket_name)\n",
    "\n",
    "instance = aws_create_ec2_instance(\n",
    "    ec2_name=\"iris-train\", \n",
    "    security_group=['sg-0d24aec64507df8b5'], \n",
    "    user_data=user_data, \n",
    "    ec2_resource=ec2_resource,\n",
    "    image_id = \"ami-062c42cbecc1d5ec0\", \n",
    "    instance_type=\"t2.medium\")\n",
    "\n",
    "print(\"instance id: \",instance.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait till instance state changes to running\n",
      "Instance State Up, waiting for boot-up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'instance is still loading retrying . . . '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance is UP & accessible on port 22, the IP address is:   3.80.171.61\n"
     ]
    }
   ],
   "source": [
    "#Provide status when instance is finally up!\n",
    "retries = 10\n",
    "retry_delay = 10\n",
    "retry_count = 0\n",
    "\n",
    "print(\"Wait till instance state changes to running\")\n",
    "instance.wait_until_running()\n",
    "instance = ec2_resource.Instance(id=instance.id)\n",
    "print(\"Instance State Up, waiting for boot-up\")\n",
    "\n",
    "waiting_status = \"instance is still loading retrying . . . \"\n",
    "dh = display(waiting_status,display_id=True)\n",
    "\n",
    "while retry_count <= retries:\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    result = sock.connect_ex((instance.public_ip_address,22))\n",
    "    if result == 0:\n",
    "        print(\"Instance is UP & accessible on port 22, the IP address is:  \",instance.public_ip_address)\n",
    "        break\n",
    "    else:\n",
    "        if len(waiting_status) < 50:\n",
    "            waiting_status += \". \"\n",
    "        else:\n",
    "            waiting_status = waiting_status[0:41]\n",
    "\n",
    "        dh.update(waiting_status)\n",
    "        time.sleep(retry_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands via SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I can tell miniconda app happens after UserData is complete, thus no installing conda environment\n",
    "\n",
    "We will execute the following command through ssh client via root access\n",
    "```\n",
    "sudo su\n",
    "while read requirement; do conda install --yes $requirement; done < /mys3bucket/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /opt/conda:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "asn1crypto                0.24.0                   py37_0  \n",
      "blas                      1.0                         mkl  \n",
      "ca-certificates           2019.10.16                    0  \n",
      "certifi                   2019.9.11                py37_0  \n",
      "cffi                      1.11.5           py37he75722e_1  \n",
      "chardet                   3.0.4                    py37_1  \n",
      "conda                     4.5.12                   py37_0  \n",
      "conda-env                 2.6.0                         1  \n",
      "cryptography              2.4.2            py37h1ba5d50_0  \n",
      "idna                      2.8                      py37_0  \n",
      "intel-openmp              2019.4                      243  \n",
      "joblib                    0.14.0                     py_0  \n",
      "libedit                   3.1.20181209         hc058e9b_0  \n",
      "libffi                    3.2.1                hd88cf55_4  \n",
      "libgcc-ng                 9.1.0                hdf63c60_0  \n",
      "libgfortran-ng            7.3.0                hdf63c60_0  \n",
      "libstdcxx-ng              8.2.0                hdf63c60_1  \n",
      "mkl                       2019.4                      243  \n",
      "mkl-service               2.3.0            py37he904b0f_0  \n",
      "mkl_fft                   1.0.15           py37ha843d7b_0  \n",
      "mkl_random                1.1.0            py37hd6b4f25_0  \n",
      "ncurses                   6.1                  he6710b0_1  \n",
      "numpy                     1.17.3           py37hd14ec0e_0  \n",
      "numpy-base                1.17.3           py37hde5b4d6_0  \n",
      "openssl                   1.0.2t               h7b6447c_1  \n",
      "pandas                    0.25.2           py37he6710b0_0  \n",
      "pip                       19.3.1                   py37_0  \n",
      "pycosat                   0.6.3            py37h14c3975_0  \n",
      "pycparser                 2.19                     py37_0  \n",
      "pyopenssl                 18.0.0                   py37_0  \n",
      "pysocks                   1.6.8                    py37_0  \n",
      "python                    3.7.0                h6e4f718_3  \n",
      "python-dateutil           2.8.1                      py_0  \n",
      "pytz                      2019.3                     py_0  \n",
      "readline                  7.0                  h7b6447c_5  \n",
      "requests                  2.21.0                   py37_0  \n",
      "ruamel_yaml               0.15.46          py37h14c3975_0  \n",
      "scikit-learn              0.21.3           py37hd81dba3_0  \n",
      "scipy                     1.3.1            py37h7c811a0_0  \n",
      "setuptools                41.6.0                   py37_0  \n",
      "six                       1.12.0                   py37_0  \n",
      "sqlite                    3.30.1               h7b6447c_0  \n",
      "tk                        8.6.8                hbc83047_0  \n",
      "urllib3                   1.24.1                   py37_0  \n",
      "wheel                     0.33.6                   py37_0  \n",
      "xz                        5.2.4                h14c3975_4  \n",
      "yaml                      0.1.7                had09818_2  \n",
      "zlib                      1.2.11               h7b6447c_3  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "commands = [\n",
    "    \"mkdir /mys3bucket/output\",\n",
    "    \"mkdir /mys3bucket/model\",\n",
    "    \"echo 'for requirement in `cat /mys3bucket/requirements.txt` ; do  conda install --yes  ${requirement}  ;  done' > /tmp/setup.sh\",\n",
    "    \"chmod +x /tmp/setup.sh\",\n",
    "    \"sudo  -i /tmp/setup.sh\",\n",
    "    \"conda list\"\n",
    "]\n",
    "\n",
    "ssh_result = exec_ssh_cmd(instance.public_ip_address,commands)\n",
    "\n",
    "#Print the last line conda list\n",
    "[print(l) for l in ssh_result[5][\"response\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Python Script<a id=Execute_Code_EC2_Script></a>\n",
    "\n",
    "Execute the train.py file!\n",
    "```\n",
    "cd /mys3bucket\n",
    "python train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the train.py file!\n",
    "```\n",
    "cd /mys3bucket\n",
    "python train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "trian model\n",
      "save model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "ssh_result = exec_ssh_cmd(\n",
    "    public_dns_name = instance.public_ip_address,\n",
    "    commands = [\"cd /mys3bucket/; python iris_train.py\"])\n",
    "\n",
    "[ print(val) for val in ssh_result[0][\"response\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for S3 to refresh then download and kill instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 564 Bytes/9.1 KiB (2.5 KiB/s) with 2 file(s) remaining\n",
      "download: s3://iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122/model/iris-labels.pkl to ../example_src/iris/model/iris-labels.pkl\n",
      "Completed 564 Bytes/9.1 KiB (2.5 KiB/s) with 1 file(s) remaining\n",
      "Completed 9.1 KiB/9.1 KiB (40.8 KiB/s) with 1 file(s) remaining \n",
      "download: s3://iris-train-12746b8f-be9f-4ef8-8c06-38323c33c122/model/iris-rf.pkl to ../example_src/iris/model/iris-rf.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "resources, bucket = aws_list_objects(\n",
    "    bucket_name=bucket_name,\n",
    "    s3_resource=s3_resource)\n",
    "\n",
    "models = [ res[\"file_name\"] for res in resources if \"model/iris\" in res[\"file_name\"]]\n",
    "models\n",
    "\n",
    "if len(models) > 0:\n",
    "    aws_download_objects(\n",
    "        path_to_src=\"../example_src/iris\",\n",
    "        bucket_name=bucket_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop \\ Terminate EC2 Instance<a id=Execute_Code_EC2_Stop></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StoppingInstances': [{'CurrentState': {'Code': 64, 'Name': 'stopping'}, 'InstanceId': 'i-0c4e0a21c3f6164a9', 'PreviousState': {'Code': 16, 'Name': 'running'}}], 'ResponseMetadata': {'RequestId': '747e5d07-623a-42fe-95f9-07bea87604fa', 'HTTPStatusCode': 200, 'HTTPHeaders': {'content-type': 'text/xml;charset=UTF-8', 'content-length': '579', 'date': 'Fri, 15 Nov 2019 20:50:26 GMT', 'server': 'AmazonEC2'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "%run AWS_functions.py\n",
    "\n",
    "response = aws_stop_ec2(\n",
    "    instance_id = instance.id, \n",
    "    ec2_client = ec2_client)\n",
    "\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model\n",
    "\n",
    "https://medium.com/@patrickmichelberger/how-to-deploy-a-serverless-machine-learning-microservice-with-aws-lambda-aws-api-gateway-and-d5b8cbead846\n",
    "\n",
    "We used a mircoserver flask and zappa to automate lambda, s3, and api gateway\n",
    "\n",
    "*Assumptions*\n",
    "* Flask app created in the project folder under api folder and works locally\n",
    "* Pickle \\ joblib file uploaded to s3\n",
    "* virtualenv installed `pip install virtualenv`\n",
    "\n",
    "1. Setup a lambda virtualenv from your project folder\n",
    "```\n",
    "cd example_src\\iris\n",
    "virtualenv lambda\n",
    "source lambda/bin/activate\n",
    "pip install flask zappa sklearn numpy scipy\n",
    "```\n",
    "2. Test your flask locally by running `python api/app.py`\n",
    "3. Initialize zappa by `zappa init`\n",
    "    * Environment: dev\n",
    "    * S3 Bucket: use default\n",
    "    * App Function: use default `api.app.app`\n",
    "    * Globally: n\n",
    "*Should look like this*\n",
    "```\n",
    "{\n",
    "    \"dev\": {\n",
    "        \"app_function\": \"api.app.app\",\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"profile_name\": \"default\",\n",
    "        \"project_name\": \"iris\",\n",
    "        \"runtime\": \"python3.7\",\n",
    "        \"s3_bucket\": \"zappa-telpd5in0\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "4) Deploy zappa \n",
    "```zappa deploy dev```<br>\n",
    "5) Test Model<br>\n",
    "```\n",
    "curl -d '{\"data\":[[4, 2.1,1,0.4], [6.2, 1,3,2]]}' -X POST https://sgj5uofirg.execute-api.us-east-1.amazonaws.com/dev\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
